\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Entropy and Mutual Information}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The smoothing distribution for the first 100 time points, using the parameters learned in the previous homework.\relax }}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}HMM -- Implementation}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Fake Parameters Inference}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}M-Step Derivation}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Deriving Parameters using EM Algorithm}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces EM Parameter Values\relax }}{6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:title}{{2.1}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Log Likelihood of Train and Test Data}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The log likelihood computed for each iteration of the EM algorithm.\relax }}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Training and Test Log Likelihood for Various Models\relax }}{8}}
\newlabel{tab:title}{{2.2}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Comparing Previous Models}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Viterbi Decoding Pseudocode}{8}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Calculate $\qopname  \relax o{arg}\qopname  \relax m{max}\limits _{z_{1:T}}P(z_{1:T} | \mathaccentV {bar}016{x}_{1:T})$\relax }}{8}}
\newlabel{alg1}{{1}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Viterbi Decoding Implementation}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Marginal Probability Computations}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9}How to Train the Model With Unknown Number of Clusters}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The training data is presented along with the cluster means, inferred from EM. The datapoints are colored based on each datapoints' Viterbi decoded cluster assignment.\relax }}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The smoothing distribution for the first 100 time points of the test set, using the parameters learned with EM.\relax }}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces The The most likely state for the first 100 time points of the test data, using Viterbi decoding and the marginal probability.\relax }}{12}}
